{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_DNN_Framework.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "FDMCBpwIXa4h",
        "i4Ru7QhOXegU"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDMCBpwIXa4h"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJ5yZ3Y1W_Qg"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.optim import Adam\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Us1Ez4uvXgBm"
      },
      "source": [
        "# Data loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JodvJTG3a-LS"
      },
      "source": [
        "from sklearn.datasets import make_classification\n",
        "X,y = make_classification(n_features=10, n_informative=6, n_classes=3, n_samples=10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFOOGR82XVSZ"
      },
      "source": [
        "`Read the data into X and y numpy arrays`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4Ru7QhOXegU"
      },
      "source": [
        "# Data preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UimGn7nYe9M"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)   #stratify = y\n",
        "\n",
        "s = StandardScaler().fit(X_train)\n",
        "X_train = s.fit_transform(X_train)\n",
        "X_test = s.fit_transform(X_test)\n",
        "\n",
        "\n",
        "# X_train = X_train.reshape(-1,1)\n",
        "# X_test = X_test.reshape(-1,1)\n",
        "\n",
        "\n",
        "X_train = torch.from_numpy(X_train)\n",
        "X_test = torch.from_numpy(X_test)\n",
        "y_train = torch.from_numpy(y_train)\n",
        "y_test = torch.from_numpy(y_test)\n",
        "\n",
        "\n",
        "tensor_train = TensorDataset(X_train, y_train)\n",
        "#tensor_test = TensorDataset(X_test, y_test)\n",
        "\n",
        "training_dataset = DataLoader(tensor_train, batch_size = 32, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HJlBiTiZ8B3"
      },
      "source": [
        "# Creating the deep learning model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGIsev3PXMjO"
      },
      "source": [
        "class mod(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        \n",
        "        super(mod, self).__init__()\n",
        "\n",
        "        self.h1 = nn.Linear(in_features=10, out_features=15)\n",
        "\n",
        "        self.bnn = nn.BatchNorm1d(num_features=15)\n",
        "        self.hn = nn.Linear(in_features=15, out_features=3)\n",
        "        \n",
        "        self.drop = nn.Dropout()\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.h1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.drop(x)\n",
        "        \n",
        "        #x = self.bnn(x)\n",
        "        x = self.hn(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDOo01fHbOkI"
      },
      "source": [
        "# Running the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uLRQQIIbPp2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "beaf672c-2bb4-44ac-b0a6-f799b44e1edf"
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "model = mod()\n",
        "model = model.to(device)\n",
        "\n",
        "opt = Adam(params = model.parameters())\n",
        "\n",
        "n_epochs = 1000\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    training_loss = 0.0\n",
        "    \n",
        "    for batch, target in training_dataset:\n",
        "\n",
        "        batch = batch.float().to(device)\n",
        "        target = target.long().to(device)\n",
        "\n",
        "        batch_prediction = model(batch)\n",
        "\n",
        "        loss = F.cross_entropy(batch_prediction,target) # loss = F.mse_loss(target, batch_prediction)\n",
        "        training_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "    #yp = test_prediction.argmax(axis=1)\n",
        "    test_prediction = model(X_test.float().to(device))\n",
        "    test_loss = F.cross_entropy(test_prediction, y_test.long()) # loss = F.mse_loss(y_test, test_prediction)\n",
        "\n",
        "    if epoch % 50 == 0:\n",
        "        print(f\"Training loss: {training_loss}, test loss: {test_loss}\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 252.8471086025238, test loss: 0.9314826726913452\n",
            "Training loss: 173.05776444077492, test loss: 0.6936585307121277\n",
            "Training loss: 170.0288051366806, test loss: 0.7060844898223877\n",
            "Training loss: 170.3834564089775, test loss: 0.6797645092010498\n",
            "Training loss: 170.21337622404099, test loss: 0.7163775563240051\n",
            "Training loss: 170.08115217089653, test loss: 0.693203866481781\n",
            "Training loss: 170.69442000985146, test loss: 0.6782982349395752\n",
            "Training loss: 170.5862332880497, test loss: 0.6778146624565125\n",
            "Training loss: 171.43888267874718, test loss: 0.6840017437934875\n",
            "Training loss: 169.52847349643707, test loss: 0.7069750428199768\n",
            "Training loss: 170.79129886627197, test loss: 0.6956117153167725\n",
            "Training loss: 171.67725339531898, test loss: 0.6897947192192078\n",
            "Training loss: 167.53862020373344, test loss: 0.6736897826194763\n",
            "Training loss: 170.26278534531593, test loss: 0.6961482763290405\n",
            "Training loss: 168.73461318016052, test loss: 0.6897329688072205\n",
            "Training loss: 170.0195991396904, test loss: 0.6989587545394897\n",
            "Training loss: 170.54677894711494, test loss: 0.6937676072120667\n",
            "Training loss: 167.8106535077095, test loss: 0.713559627532959\n",
            "Training loss: 171.9801842868328, test loss: 0.7076643705368042\n",
            "Training loss: 167.60075256228447, test loss: 0.6967231035232544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBgb6uLBlXeS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}