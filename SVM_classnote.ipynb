{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0fbbe9b0"
   },
   "source": [
    "# Support Vector Machine (SVM)\n",
    "\n",
    "Support vector machine (SVM) works by creating either optimal hyperplanes (for classificaion task) or optimal hypertube (for regression task). In both the cases, SVM takes the help of geometry to determine the distance of a point from the hyperplane or from the axis of the hypertube. We would first focus on the classifcation task and later move on to the regression task. SVM is also capable of detecting multivariate outliers (called novelty detection). SVM also takes into consideration the concept of kernel space to map non-linearity in the dataset. A brief introduction about the kernel space will be given when it will be necessary. But one thing that is to be kept in mind always that ***SVM, in essence, is a linear classifier or a linear regressor***. It is the application of kernel space which helps SVM to map nonlinearity in the dataset.\n",
    "\n",
    "<h3>SVM classifier</h3>\n",
    "\n",
    "Consider that there are two classes (red and green). The two classes are linearly separable as shown in the image below. So, which linear plane is most appropriate to separate the classes? The answer is the continous one because it is passing through the middle portion of the empty space between the two classes. The dotted lines are improper because slight addition of noise would produce miss-classification of points lying closer the the dotted lines. Hence, from classification point of view, the continuous line is considered the most optimal separating hyperplane \n",
    "\n",
    "*(Note: a hyperplane is a plane in a 3+ dimensional feature space. In 2D space, the optimal hyperplane is just a line)*\n",
    "\n",
    "\n",
    "|![Figure 1](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcS1JM6z43agKKvuRxDlIAYiClKFmug15z7EYA&usqp=CAU)|\n",
    "|:--:|\n",
    "|*Figure 1*|\n",
    "|*Source:https://stackabuse.com/implementing-svm-and-kernel-svm-with-pythons-scikit-learn*|\n",
    "\n",
    "\n",
    "The next problem is to specify criteria to define the hyperplane. It is to be understood that the hyperplane should be defined after taking into consideration of the space separating the two classes only. Hence, only those points which are lying at the edge of the sepating space becomes most important to decide the location of the optimal hyperplane. To understand this, have alook at the image below:\n",
    "\n",
    "|![Figure 2](https://www.baeldung.com/wp-content/uploads/sites/4/2020/10/svm-all.png)|\n",
    "|:--:|\n",
    "|*Figure 2: Margin and optimal hyperplane*|\n",
    "\n",
    "\n",
    "As per the above image, only 2 blue dots and one green dot are lying at the edge of the separating space. Hence, to decide the optimal hyperplane (denoted by the red line), only these three points will take entire reponsibility. In other words, these three points are ***supporting*** the hyperplane to keep it in place. Since each point can be considered as a vector in the input space and these three vectors are supporting the optimal hyperplane hence the algorithm is called the support vector machine. \n",
    "\n",
    "<h3>How to locate the support vectors?</h3>\n",
    "\n",
    "To answer this question, some amount of mathematical formulations are required. We need to start with the distance of a point from a plane. Have a look at the image below:\n",
    "\n",
    "|![Figure 3](https://drive.google.com/uc?id=1UgENW0iWEPvqXPudMJkyC3jNPBL0yW4Z)|\n",
    "|:--:|\n",
    "|*Figure 3: Distance between a plane and a point*|\n",
    "\n",
    "\n",
    "Let the plane be defined by $Z(x)=w^Tx+b=0$ where $w$ is the coefficient of the plane and $b$ in the intercept. We can consider any point $C$ which is not lying on the the plane. The distance between the point $C$ and the plane is the perpendicular line on the plane dropped from the point $C$. Let the point of intersection be $D$. We can also consider another arbitrary point $E$ lying on the plane, not coinciding with $D$. Now, since both $D$ and $E$ are lying on the same plane, \n",
    "$$w^Tx_D+b=w^Tx_E+b\\\\ $$\n",
    "$$\\therefore w^T(x_D-x_E)=0\\\\ $$\n",
    "$$Or\\ w^Tv=0,\\ because\\ \\vec v=\\vec x_D-\\vec x_E\\\\ $$\n",
    "This means that the weight vector $\\vec w$ is perpendicular to the plane which is denoted by the red arrow. The unit vector $\\hat w$ is given by, $\\hat w=\\frac{\\vec w}{\\lVert w \\rVert}$. Now the perpendicular distance $h$ is actually the projection of the $\\vec u$ vector on the $\\hat w$ vector. Hence,\n",
    "\n",
    "$$\\begin{align}\n",
    "h&=\\frac{w}{\\lVert w\\rVert}(x_C-x_E)\\\\ \n",
    "&=\\frac{1}{\\lVert w\\rVert}(w^Tx_C-w^Tx_E)\\\\ \n",
    "&=\\frac{1}{\\lVert w\\rVert}(w^Tx_C+b),\\ because\\ w^Tx_E+b=0\\ for\\ x_E\\ lying\\ on\\ the\\ plane\\\\ \n",
    "&=\\frac{w^Tx_C+b}{\\lVert w\\rVert}\n",
    "\\end{align}\\\\ $$\n",
    "\n",
    "<h3>Canonical Hyperplane</h3>\n",
    "\n",
    "We have seen that $h=\\frac{w^Tx_C+b}{\\lVert w\\rVert}$ and in this expression, the numerator portion is the absolute distance value and the denominator is the normalizing factor. In SVM, data points which are lying above the optimal hyperplane are given a label $+1$ and the ones lying below the hyperplane are given a label $-1$. Moreover, SVM focuses only on locating the support vectors. Hence, for all practical purposes, we can assume that the feature space can be suitably scaled such that, for support vectors, the numerator term turns out to be $1$. That is, if the label of one of the support vector is $y^*$ and the support vector is $x^*$, we can choose a suitable scaling factor $s$ such that \n",
    "\n",
    "$$sy^*(w^Tx^*+b)=1\\\\ $$\n",
    "$$\\therefore \\frac{sy^*(w^Tx^*+b)}{\\lVert w\\rVert}=\\frac{1}{\\lVert w\\rVert}\\\\ $$\n",
    "$$Or,\\ sy^*h(x^*)=\\frac{1}{\\lVert w\\rVert}\\\\ $$\n",
    "\n",
    "So, if we assume that the scaling has already been done so that $y^*h(x^*)=1$ for the support vector $x^*$, what we get is a canonical form of the hyperplane and the distance between the hyperplane and the support vector is denoted by $\\frac{1}{\\lVert w\\rVert}$. This distance is called the margin in SVM. Thus margin $M$ is denoted as \n",
    "\n",
    "$$M=\\frac{1}{\\lVert w\\rVert}\\\\ $$\n",
    "\n",
    "If we assume that two maginal planes are going through the support vectors related to two different classes $y=[-1,1]$ the optimal hyperplane must go through the middle of the two marginal planes. This is shown in the figure shown below. \n",
    "\n",
    "|![Figure 4](https://www.researchgate.net/profile/Feng-Li-Lian/publication/267556836/figure/fig1/AS:295583391731712@1447483980381/A-large-margin-can-be-achieved-by-the-SVM.png)|\n",
    "|:--:|\n",
    "|*Figure 4:Understanding the margin*|\n",
    "\n",
    "There are two different hyperplanes are shown. The second one is better because the marginal planes (shown by the dotted lines) are more widely separated than the first one. This is where SVM needs to solve an optimization problem as shown below:\n",
    "\n",
    "$$w^*=\\operatorname*{argmax}_{w}\\frac{2}{\\lVert w\\rVert},\\ because\\ width\\ between\\ marginal\\ plnes\\ is\\ \\frac{2}{\\lVert w\\rVert}$$\n",
    "$$Subjected\\ to\\ y_i(w^Tx_i+b) \\ge 1\\\\ $$\n",
    "\n",
    "An equivalent objective function is:\n",
    "\n",
    "$$min \\frac{1}{2}{\\lVert w\\rVert}_2^2 = min \\frac{1}{2}w^Tw\\\\ $$\n",
    "$$Subjected\\ to\\ y_i(w^Tx_i+b) \\ge 1\\\\ $$\n",
    "\n",
    "The above optimization problem can be solved using the method of Lagranges Multiplier by creating a new combined objective function as shown below:\n",
    "\n",
    "$$L = \\frac{1}{2}w^Tw - \\sum_{i=1}^n\\alpha_i (y_i(w^Tx_i+b) - 1)$$\n",
    "\n",
    "To optimize the objective function, $L$ needs to be partially differentiated with respect to both $w$ and $b$ and equated to 0 respectively. Therefore,\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w}=w-\\sum_{i=1}^n\\alpha_i y_i x_i = 0,\\ and\\\\ $$\n",
    "$$\\frac{\\partial L}{\\partial b}=\\sum_{i=1}^n\\alpha_i y_i = 0,\\\\ $$\n",
    "$$\\therefore w=\\sum_{i=1}^n\\alpha_i y_i x_i,\\ and\\ \\sum_{i=1}^n\\alpha_i y_i = 0\\\\ $$\n",
    "\n",
    "Putting these values in the original combined objective function $L$ we get,\n",
    "\n",
    "$$\\begin{align}L&=\\frac{1}{2}w^Tw - \\sum_{i=1}^n(w^T\\alpha_iy_ix_i)-\\sum_{i=1}^n\\alpha_iy_i+\\sum_{i=1}^n\\alpha_i\\\\ \n",
    "&=\\frac{1}{2}w^Tw - w^T\\sum_{i=1}^n\\alpha_iy_ix_i - 0 + \\sum_{i=1}^n\\alpha_i\\\\ \n",
    "&=\\frac{1}{2}w^Tw-w^Tw+\\sum_{i=1}^n\\alpha_i\\\\ \n",
    "&=\\sum_{i=1}^n\\alpha_i - \\frac{1}{2}w^Tw\\\\ \n",
    "&=\\sum_{i=1}^n\\alpha_i - \\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^n\\alpha_i\\alpha_jy_iy_jx_i^Tx_j\n",
    "\\end{align}\\\\ $$ \n",
    "$$Subjected\\ to\\ \\alpha_i \\ge 0, \\forall i\\\\ $$\n",
    "\n",
    "This new objective function is quadratic in nature and it is the dual of the original objective function havine both $w$ and $b$. As per the concept of optimization, at the point of optimization, both primal and dual objective function would provide same result and since the dual form in this situation is simpler than the primal one (because it has only one set of parameters i.e. $\\alpha_i$ compared to $w$ and $b$ as in case of primal problem and simpler constrained function), this objective function is optimized using standard quadratic optimization technique which is out side the scope of this note. \n",
    "\n",
    "Once all the $\\alpha_i$ values are found out, it will be seen that only for a few $\\{x_i,y_i\\}$ the $\\alpha_i$ values are postive and rest all are zeros. An important condition during the optimization process is Karush Kuhn Tucker (KKT) condition which states that for all $i$ values, \n",
    "\n",
    "$$\\alpha_i(y_i(w^Tx_i+b)-1)=0$$\n",
    "\n",
    "This condition is very significant because:\n",
    "* for $\\alpha_i > 0$, $y_i(w^Tx_i+b)-1=0$, i.e., $y_i(w^Tx_i+b)=1$ suggesting the $x_i$ is a support vector\n",
    "* for $\\alpha_i = 0$, $y_i(w^Tx_i+b)-1 > 0$, i.e. $y_i(w^Tx_i+b)>1$ suggesting that $x_i$ is not a support vector\n",
    "\n",
    "Once the support vectors are identified, the weight vector is found out by the formula:\n",
    "\n",
    "$$w=\\sum_{i,\\alpha_i>0}\\alpha_iy_ix_i$$\n",
    "\n",
    "Once $w$ is found out we have,\n",
    "\n",
    "$$y_i(w^Tx_i+b)=1$$\n",
    "$$Or,\\ b_i = \\frac{1}{y_i}-w^Tx_i=y_i-w^Tx_i,\\ because\\ y_i\\in\\{-1,1\\}$$\n",
    "\n",
    "The final bias value $b$ is determined by taking averages of all the individual $b_i$ values, i.e.\n",
    "\n",
    "$$b=avg_{\\alpha_i>0}\\{b_i\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DF9XSftFUXiN"
   },
   "source": [
    "<h3><b>Soft Margin</b></h3>\n",
    "\n",
    "The SVM which has been discussed above deals with hard margin, i.e. between two marginal planes, no data point should be lying. However, in real life situation such hard and fast restrictions are difficult to maintain. Hence, we can allow some data points within the marginal planes (and even missclassifcations too) with penalty terms associated with it. The image shown below gives the idea of soft margin. In this figure $\\zeta_i$ (zeta) is the slack associated with each and every data point which are lying in the wrong side of the associated marginal plane. \n",
    "\n",
    "|![Figure 4](https://www.researchgate.net/profile/Lang-Tran/publication/327015448/figure/fig2/AS:659696117633025@1534295219130/SVM-with-soft-margin-kernel-with-different-cases-of-slack-variables.png)|\n",
    "|:--:|\n",
    "|*Figure 4: Soft margin in SVM*|\n",
    "|*Source: AUTOMATIC HEART DISEASE PREDICTION USING FEATURE SELECTION AND DATA MINING TECHNIQUE*|\n",
    "\n",
    "The associated condition becomes $$y_i(w^Tx_i+b)\\ge1-\\zeta_i$$\n",
    "\n",
    "Ideally, $\\zeta$ should be as small as possible. But if we allow $\\zeta$ to have non-zero values, we deal with **soft margin**. The associated loss function with soft margin is\n",
    "\n",
    "$$\\operatorname*{min}_{w,b,\\zeta_i}\\left\\{ \\frac{\\lVert w\\rVert^2}{2}+C\\sum_{i=1}^n\\zeta_i^k\\right\\}\\\\ $$\n",
    "$$Linear\\ constraint\\ :y_i(w^Tx_i+b)\\ge1-|\\zeta_i|\\ \\ \\ \\forall x_i\\in D\\\\ $$\n",
    "$$with\\ |\\zeta_i|\\ge0\\ \\ \\forall x_i\\in D\\\\ $$\n",
    "\n",
    "Here, one thing to be understood is that if $\\zeta_i\\le1$, it will still be a correct classification but if $\\zeta_i\\ge1$ then the $i^{th}$ data point would be a missclassification. In the above equation, power of $\\zeta$ is put as $k$ which varies from 1 to 2. With $k=1$, it is called the **hinge loss** and with $k=2$ it is called the **quadratic loss**. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UQ7KJ9NAsSiq"
   },
   "source": [
    "For both hinge and quadratic loss, similar process is applied as mentioned earlier, i.e, creating a Langrangian loss function and optimizing it using Lagrangian multiplier method. For the sake of information, the loss functions are shown below:\n",
    "\n",
    "$$Hinge\\ Loss:\\ L_h=\\frac{\\lVert w\\rVert^2}{2}+C\\sum_{i=1}^n\\zeta_i-\\sum_{i=1}^n\\alpha_i(y_i(w^Tx_i+b)-1+\\zeta_i)-\\sum_{i=1}^n\\beta_i\\zeta_i$$\n",
    "\n",
    "$$Quadratic\\ Loss:\\ L_q=\\frac{\\lVert w\\rVert^2}{2}+C\\sum_{i=1}^n\\zeta_i^2-\\sum_{i=1}^n\\alpha_i(y_i(w^Tx_i+b)-1+\\zeta_i)\\\\ $$\n",
    "\n",
    "Observe that in $L_q$ the term $\\sum_{i=1}^n\\beta_i\\zeta_i$ is not appearing. It is because the value of $C\\sum_{i=1}^n\\zeta_i^2$ is always positive and hence its minimum value can be $0$. Hence, in this case, the objective function is satisfied even if $\\zeta_i\\le0$. However, the same is not true in case of hinge loss because the term $C\\sum_{i=1}^n\\zeta_i$ can have both positive and negative values. That is why, following KKT condition, another lagrangian multiplier $\\beta_i$ was needed in case of hinge loss. Further mathematical derivations are similar to what has already been shown above and hence it is kept as a task for the students."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nqi_fh1J2FQD"
   },
   "source": [
    "<h3><b>Mapping non-linearity</b></h3>\n",
    "\n",
    "SVM is, inherently, a linear classifier. So, to understand how SVM maps the non-linearity, we need to understand the basic concept of **Kernel space**. Let us assume that the input space is having only two variables $x_1\\ and\\ x_2$. We can think about a mapping function which projects the data points in the input space to a new space which is created by some combination of the dimensions of the input space, i.e. $x_1$ and $x_2$. Let $n_i$ and $n_j$ be two data points in the input space. Therefore $n_i=[x_{1i},x_{2i}]$ and $n_j=[x_{1j},x_{2j}]$. Let the mapping function creates the following new dimensions based on the input dimension $x_1$ and $x_2$.\n",
    "\n",
    "$$\\phi(x)=\\left[1, \\sqrt 2x_1, \\sqrt 2x_2, \\sqrt 2x_1x_2,  x_1^2, x_2^2  \\right]$$\n",
    "\n",
    "Hence, if $N_i$ and $N_j$ are the data points in the new feature space corresponding to $n_i$ and $n_j$ data point in the input space respectively, then\n",
    "\n",
    "$$N_i=\\phi(n_i)=\\left[ 1, \\sqrt 2x_{1i}, \\sqrt 2x_{2i}, \\sqrt 2x_{1i}x_{2i},x_{1i}^2,x_{2i}^2\\right]\\\\ $$\n",
    "$$And\\\\ $$\n",
    "$$N_j=\\phi(n_j)=\\left[ 1, \\sqrt 2x_{1j}, \\sqrt 2x_{2j}, \\sqrt 2x_{1j}x_{2j},x_{1j}^2,x_{2j}^2\\right]\\\\ $$\n",
    "\n",
    "So, if we need to find similarities between $N_i$ and $N_j$ in the new feature space, we would find the dot product between $N_i$ and $N_j$ (denoted by $\\langle N_i,N_j\\rangle$) as follows:\n",
    "\n",
    "$$\\langle N_i,N_j\\rangle=N_i^TN_j\\ \\ \\ (N_i,N_j\\ are\\ column\\ vectors)\\\\ $$\n",
    "$$\\begin{align}\n",
    "N_i^TN_j &=\\phi (n_i)^T\\phi (n_j)\\\\ \n",
    "&= \\left[1+ 2x_{1i}x_{1j}+ 2x_{2i}x_{2j}+2x_{1i}x_{2i}x_{1j}x_{2j}+x_{1i}^2x_{1j}^2+x_{2i}^2x_{2j}^2\\right]\\\\ \n",
    "&= \\left[1+(x_{1i}x_{1j}+x_{2i}x_{2j})\\right]^2\\\\ \n",
    "&= \\left[1+n_i^Tn_j\\right]^2\\\\ \n",
    "&= \\left[1+\\langle n_i, n_j\\rangle \\right]^2\\ \\ \\ (a\\ polynomial\\ kernel\\ function\\ of\\ order\\ 2)\n",
    "\\end{align}$$\n",
    "\n",
    "The new feature space with similarity measures is called the kernel space. The above expression is quite interesting because, as per the equation, if we need to find the similarity between two points in the new feature space, we need to simply find the dot product in the input space and then do necessary calculations based on the dot product in the input space only. This is something which is called **the kernel trick** in the literature. So, what is a kernel space? A kernel space can be considered as a generalized dot product space which signifies similarities among data points in a high dimensional space. In the above explanation, the $2D$ space is projected into the $6D$ space. The natural question comes out here as why to project a low dimensional input space to high dimensional space. The answer is, in low dimension if the pattern looks quite non-linear, in high dimension, it might turn out to be linear. But, there are problems associated with it such as:\n",
    "* With increased dimension, we face **the curse of dimensionality**. \n",
    "* Calculating coordinates in high dimension is computationaly very expensive\n",
    "\n",
    "Kernel space tries to solve these problems by looking at the data from a completely different perspective, i.e. similarity matrix. This is where the dot product at the kernel space comes into play. So, if the dot product in the kernel space can be finally evaluated as a function of dot product in the input space, practically, we need not calculate the coordinates of the points in the kernel space. That is why, not every function can be a kernel function. A kernel function is a function of the dot product of the data points in the input space which exactly represents the dot product of the same data points in the kernel space after some space mapping function $\\phi(x)$. The kernel trick mentioned above is critical for the success of kernel based algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JuyI6OU7Yuaz"
   },
   "source": [
    "<h3>Simple Example</h3>\n",
    "\n",
    "Let $\\phi(x)$ represents a point $x$ in the kernel space where $x$ is lying in the input space. If there are $n$ such points, there will be corresponding points $[\\phi(x_1),\\phi(x_2),\\phi(x_3),...\\phi(x_n)]$. So, what is the mean of the data points in the kernel space? Let us try to find the solution.\n",
    "\n",
    "Let $\\mu$ be the required mean. Then $\\mu=\\frac{1}{n}\\sum_{i=1}^n\\phi(x_i)$,\n",
    "\n",
    "$$\\begin{align}\n",
    "\\therefore \\mu^2&=\\left (\\frac{1}{n}\\sum_{i=1}^n\\phi(x_i) \\right )\\left (\\frac{1}{n}\\sum_{i=1}^n\\phi(x_i) \\right )\\\\ \n",
    "&=\\frac{1}{n^2}\\sum_{i=1}^n\\sum_{j=1}^n\\phi(x_i)^T\\phi(x_j)\\\\ \n",
    "&=\\frac{1}{n^2}\\sum_{i=1}^n\\sum_{j=1}^nK(x_i,x_j)\\ \\ \\ (where\\ K\\ is\\ the\\ suitable\\ kernel\\ function)\\\\ \n",
    "&=\\frac{1}{n^2}\\times Sum\\ of\\ Kernel\\ Matrix\n",
    "\\end{align}$$\n",
    "\n",
    "\n",
    "<b>Find mean with linear kernel</b>\n",
    "\n",
    "In case of linear kernel,\n",
    "$$\\phi(x_i)^T\\phi(x_j)=\\alpha x_i^Tx_j+c$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9bf24cb1",
    "outputId": "e1680efa-c1eb-478b-c9ee-73d971c167af"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  3,  4,  5],\n",
       "       [ 2,  4,  6,  8, 10],\n",
       "       [ 3,  6,  9, 12, 15],\n",
       "       [ 4,  8, 12, 16, 20],\n",
       "       [ 5, 10, 15, 20, 25]])"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let the 5 values are [1,2,3,4,5]. Linear mean is given by (1+2+3+4+5)/5=3\n",
    "# Let us try to find this mean value with Kernel method\n",
    "import numpy as np\n",
    "V = np.array([1,2,3,4,5]).reshape(5,1)\n",
    "K_v_linear = V.dot(V.T) # Linear kernel with c=0 and alpha=1\n",
    "K_v_linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "46d98aa3-c4db-4590-b2ae-e84aa86e4e77",
    "outputId": "4d8e6327-0736-46b9-c09d-88b7fc665741"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_sq = (1/5**2)*(K_v_linear.sum())\n",
    "mean = np.sqrt(mean_sq)\n",
    "mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVC9xsklgJTa"
   },
   "source": [
    "<b>Find mean with polynomial kernel of order 2</b>\n",
    "\n",
    "In case of polynomial kernel of order 2,\n",
    "$$\\ \\\\ \\phi(x_i)^T\\phi(x_j)=(c+\\alpha x_i^Tx_j)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u0u9lBgufuTZ",
    "outputId": "23f29089-fc72-40d8-bbc7-6cf7f0b0b4af"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  4,   9,  16,  25,  36],\n",
       "       [  9,  25,  49,  81, 121],\n",
       "       [ 16,  49, 100, 169, 256],\n",
       "       [ 25,  81, 169, 289, 441],\n",
       "       [ 36, 121, 256, 441, 676]])"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K_v_poly = (1 + V.dot(V.T))**2 # Assuming c=1 and alpha=1\n",
    "K_v_poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fo3ki_vng6p2",
    "outputId": "4309391a-7aad-4257-a28c-1c38b0131371"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.832159566199232"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_sq_poly = (1/5**2)*(K_v_poly.sum())\n",
    "mean_poly = np.sqrt(mean_sq_poly)\n",
    "mean_poly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qOls-bF-hWtI"
   },
   "source": [
    "In the above expression, the mean calculated is not coming from a univariate Gaussian distribution but from a multivariate (probably non Gaussian) distribution. There are several kernel functions available for mapping non-linearity.\n",
    "\n",
    "However, commonly used kernel functions are:\n",
    "* Linear kernel (for understanding purpose mainly) $\\ \\ \\ [K(x,y)=x^Ty+c]$\n",
    "* Polynomial kernel of order $d$ $\\ \\ \\ \\ [K(x,y)=(c+\\alpha x^Ty)^d]$\n",
    "* Sigmoid kernel $\\ \\ \\ \\ [K(x,y)=tanh(c+\\alpha x^Ty)]$\n",
    "* Radial basis function kernel $\\ \\ \\ \\ \\left[K(x,y)=exp\\left(-\\frac{\\lVert x-y\\rVert^p}{2\\sigma^2}\\right)\\right] \\begin{cases}\n",
    "p=1, & \\text{Exponetial kernel function} \\\\ \n",
    "p=2, & \\text{Gaussian kernel function}\\end{cases}$\n",
    "\n",
    "More about various other kernel functions can be found <a href='http://crsouza.com/2010/03/17/kernel-functions-for-machine-learning-applications/'>here</a>.\n",
    "\n",
    "Thus, if linear model is incapable of extracting the pattern properly, we can hypothetically transfer the data to a kernel space (whenever possible), do all the mathematical calculations in the kernel space to get the solutions in terms of some kernel function in the input space. This way, a linear model can map non-linear pattern. Linear models such as KMeans clustering, Discriminant Analysis, Logistic Regression, PCA and many more are successfully kernelized to map non-linear patterns in data. SVM also uses the same kernel method to map the non-linearlity. Kernel methods can be adopted where we find that the mathematical expression contains a dot product of data points. \n",
    "\n",
    "This class note is ended here. Students can find another good presentation <a href='https://www.seas.upenn.edu/~cis519/spring2019/lectures/07_SVMs.pdf'>here<a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AzirYy6RkfnO"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "SVM classnote.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
